{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a888077-5b02-429f-8fd6-1d8c2229e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2126\n",
      "baseline value                                            0.243235\n",
      "accelerations                                            -0.378693\n",
      "fetal_movement                                            0.062202\n",
      "uterine_contractions                                     -0.270586\n",
      "light_decelerations                                      -0.078528\n",
      "severe_decelerations                                           NaN\n",
      "prolongued_decelerations                                  0.351735\n",
      "abnormal_short_term_variability                           0.483140\n",
      "mean_value_of_short_term_variability                     -0.030078\n",
      "percentage_of_time_with_abnormal_long_term_variability    0.478808\n",
      "mean_value_of_long_term_variability                      -0.072185\n",
      "histogram_width                                          -0.217677\n",
      "histogram_min                                            -0.053095\n",
      "histogram_max                                             0.000216\n",
      "histogram_number_of_peaks                                -0.106558\n",
      "histogram_number_of_zeroes                               -0.002110\n",
      "histogram_mode                                            0.202433\n",
      "histogram_mean                                            0.280322\n",
      "histogram_median                                          0.075856\n",
      "histogram_variance                                        0.035657\n",
      "histogram_tendency                                             NaN\n",
      "Outcome                                                   1.000000\n",
      "dtype: float64\n",
      "1655\n",
      "471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STUDENT018\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16361637\n",
      "0.039452665\n",
      "0.055589207\n",
      "0.030210951\n",
      "0.0\n",
      "0.15426126\n",
      "0.1664445\n",
      "0.021587783\n",
      "0.07321094\n",
      "0.021267923\n",
      "0.01470931\n",
      "0.01915477\n",
      "0.026327614\n",
      "0.016911361\n",
      "0.036992695\n",
      "0.046137966\n",
      "0.068591766\n",
      "0.025282377\n",
      "0.020250501\n",
      "0.0\n",
      "[ 9.30620884  5.48612418  7.72977422  3.79045155  1.          7.42944497\n",
      " 18.80809031  5.83795861 17.41956726  7.89228598 11.5738476  12.69285042\n",
      " 12.73659454 10.31373471  2.31608655 16.37041392 15.39651929 12.35536218\n",
      " 11.24811853  1.        ]\n",
      "0.9352789624002134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '326.46592471496933' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '182.43354954612926' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '217.03455946000858' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '231.98275797998437' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '95.53009996854395' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '5.291502622129181' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '82.46211251235322' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '128.1717597600969' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '78.74007874011811' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '273.14098923449774' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
      "C:\\Users\\STUDENT018\\AppData\\Local\\Temp\\ipykernel_15400\\3418656851.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '4.242640687119286' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9390040303056196\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy.linalg import norm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from decimal import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import math\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "number_runnings = 50\n",
    "fetal = pd.read_csv(\"fetal.csv\")\n",
    "fetal[\"Outcome\"].value_counts()\n",
    "\n",
    "Y = fetal['Outcome']\n",
    "target_names=[\"0\", \"1\"]\n",
    "X = fetal[['accelerations',\t'fetal_movement',\t'uterine_contractions',\t'light_decelerations',\t'severe_decelerations',\t'prolongued_decelerations',\n",
    "              'abnormal_short_term_variability',\t'mean_value_of_short_term_variability',\t'percentage_of_time_with_abnormal_long_term_variability',\n",
    "              'mean_value_of_long_term_variability',\t'histogram_width',\t'histogram_min',\t'histogram_max',\t'histogram_number_of_peaks',\n",
    "              'histogram_number_of_zeroes',\t'histogram_mode',\t'histogram_mean',\t'histogram_median',\t'histogram_variance',\t'histogram_tendency'\t]]\n",
    "maximums = np.amax(np.array(X), axis=0)\n",
    "X_featurenames = X.columns\n",
    "noOfInstances = fetal.shape[0]\n",
    "print(noOfInstances)\n",
    "df = pd.DataFrame(fetal)\n",
    "corr = df.corrwith(df[\"Outcome\"])\n",
    "print(corr)\n",
    "\n",
    "counter = Counter(Y)\n",
    "print(counter[0])\n",
    "print(counter[1])\n",
    "\n",
    "feature_names = ['accelerations',\t'fetal_movement',\t'uterine_contractions',\t'light_decelerations',\t'severe_decelerations',\t'prolongued_decelerations',\n",
    "              'abnormal_short_term_variability',\t'mean_value_of_short_term_variability',\t'percentage_of_time_with_abnormal_long_term_variability',\n",
    "              'mean_value_of_long_term_variability',\t'histogram_width',\t'histogram_min',\t'histogram_max',\t'histogram_number_of_peaks',\n",
    "              'histogram_number_of_zeroes',\t'histogram_mode',\t'histogram_mean',\t'histogram_median',\t'histogram_variance',\t'histogram_tendency']\n",
    "results = pd.read_excel('resultsRankingsFetalMSPMS_NoMinMax.xlsx')\n",
    "noOfIterations = 20\n",
    "\n",
    "model = XGBClassifier()\n",
    "# fit the model\n",
    "model.fit(X, Y)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i in range(len(feature_names)):\n",
    "    print(importance[i])\n",
    "\n",
    "df_LIME = pd.read_excel('rankingsFetalLIME.xlsx')\n",
    "df_LIME = df_LIME[df_LIME.columns[1:]]\n",
    "W_LIME = []\n",
    "\n",
    "df_Anchor = pd.read_excel('rankingsFetalAnchor.xlsx')\n",
    "df_Anchor = df_Anchor[df_Anchor.columns[1:]]\n",
    "W_Anchor = []\n",
    "\n",
    "df_TreeShap = pd.read_excel('rankingsFetalTreeShap.xlsx')\n",
    "df_TreeShap = df_TreeShap[df_TreeShap.columns[1:]]\n",
    "df_KernelShap = pd.read_excel('rankingsFetalKernelShap.xlsx')\n",
    "df_KernelShap = df_KernelShap[df_KernelShap.columns[1:]]\n",
    "W_Shap = []\n",
    "W_all = []\n",
    "\n",
    "for instance in range (0, noOfInstances):\n",
    "    m = pd.DataFrame(columns = ['A', 'L', 'S'])\n",
    "    line_Anchor  = [results.iloc[instance, 1], results.iloc[instance, 2], results.iloc[instance, 3]]\n",
    "    m.loc[0] = line_Anchor\n",
    "    line_LIME  = [results.iloc[instance, 4], results.iloc[instance, 5], results.iloc[instance, 6]]\n",
    "    m.loc[1] = line_LIME\n",
    "    line_Shap  = [results.iloc[instance, 7], results.iloc[instance, 8], results.iloc[instance, 9]]\n",
    "    m.loc[2] = line_Shap\n",
    "    A = []\n",
    "    for i in range(0, 3):\n",
    "        s = 0\n",
    "        for j in range(0, 3):\n",
    "            s = s + m.iloc[i, j] + m.iloc[j, i]\n",
    "        s = s / 6\n",
    "        A.append(s)\n",
    "       \n",
    "    data_LIME = []\n",
    "    data_Anchor = []\n",
    "    data_Shap = []\n",
    "    for i in range(noOfIterations):\n",
    "        data_LIME.append(df_LIME.loc[instance * noOfIterations + i].values.flatten().tolist())\n",
    "        data_Anchor.append(df_Anchor.loc[instance * noOfIterations + i].values.flatten().tolist())\n",
    "        \n",
    "    for i in range(int(noOfIterations / 2)):\n",
    "        data_Shap.append(df_KernelShap.loc[instance].values.flatten().tolist())\n",
    "        data_Shap.append(df_TreeShap.loc[instance].values.flatten().tolist())\n",
    "        \n",
    "    M_LIME  = pd.DataFrame(data_LIME, columns = feature_names)\n",
    "    w_LIME = []\n",
    "    M_Anchor  = pd.DataFrame(data_Anchor, columns = feature_names)\n",
    "    w_Anchor = []\n",
    "    M_Shap  = pd.DataFrame(data_Shap, columns = feature_names)\n",
    "    w_Shap = []\n",
    "    A = [1, 1, 1]\n",
    "    for i in range(noOfIterations):\n",
    "        x = [0] * len(feature_names)\n",
    "        for k in range(0, 3):\n",
    "            x = x + A[k] * M_LIME.loc[i].values.flatten()\n",
    "        y = sum(A)\n",
    "        w = [0] * len(feature_names)\n",
    "        for k in range(len(feature_names)):\n",
    "            w[k] = x[k] / y\n",
    "        w_LIME.append(w)\n",
    "\n",
    "        x = [0] * len(feature_names)\n",
    "        for k in range(0, 3):\n",
    "            x = x + A[k] * M_Anchor.loc[i].values.flatten()\n",
    "        y = sum(A)\n",
    "        w = [0] * len(feature_names)\n",
    "        for k in range(len(feature_names)):\n",
    "            w[k] = x[k] / y\n",
    "        w_Anchor.append(w)\n",
    "\n",
    "        x = [0] * len(feature_names)\n",
    "        for k in range(0, 3):\n",
    "            x = x + A[k] * M_Shap.loc[i].values.flatten()\n",
    "        y = sum(A)\n",
    "        w = [0] * len(feature_names)\n",
    "        for k in range(len(feature_names)):\n",
    "            w[k] = x[k] / y\n",
    "        w_Shap.append(w)\n",
    "        \n",
    "    averageLIME = np.mean(w_LIME, axis=0)\n",
    "    W_LIME.append(averageLIME)\n",
    "    averageAnchor = np.mean(w_Anchor, axis=0)\n",
    "    W_Anchor.append(averageAnchor)\n",
    "    averageShap = np.mean(w_Shap, axis=0)\n",
    "    W_Shap.append(averageShap)\n",
    "    averageMatrix = [averageShap]\n",
    "    average = np.mean(averageMatrix, axis=0)\n",
    "    sum_average = sum(average)\n",
    "    sum_corr = sum(corr) - 1\n",
    "    min = np.min(average)\n",
    "    max = np.max(average)\n",
    "    for i in range(0, len(feature_names)):\n",
    "        #average[i] = (average[i] / sum_average + corr.iloc[i] / sum_corr) / 2\n",
    "        #average[i] = average[i] * corr.iloc[i]\n",
    "        #average[i] = average[i] * (corr.iloc[i] / sum_corr)\n",
    "        #average[i] = average[i] + corr.iloc[i]\n",
    "        average[i] = average[i]\n",
    "    W_all.append(average)\n",
    "    # print(averageLIME)\n",
    "    # print(averageAnchor)\n",
    "    # print(averageShap)\n",
    "    # print(average)\n",
    "    # print(\"============\")\n",
    "average_all = np.mean(W_all, axis=0)\n",
    "print(average_all)\n",
    "\n",
    "\n",
    "def compute_weights(distance):\n",
    "    weights = []\n",
    "    for i in range(len(distance)): \n",
    "        weights.append(norm(W_all[i]) * distance[i])\n",
    "    return weights\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, shuffle = True)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "    \n",
    "# knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "# knn.fit(X_train,Y_train) \n",
    "# Y_pred = knn.predict(X_test)\n",
    "# print(accuracy_score(Y_test,Y_pred))\n",
    "# print(f1_score(Y_test,Y_pred))\n",
    "\n",
    "#X2 = scaler.fit_transform(X)\n",
    "scale_pos_weight = counter[0] / counter[1]\n",
    "model = xgb.XGBClassifier(scale_pos_weight = scale_pos_weight)\n",
    "# model.fit(X_train, Y_train)\n",
    "# pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(Y_test, pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "scores = cross_val_score(model, X, Y, cv = 10, scoring='roc_auc')\n",
    "print(np.mean(scores))\n",
    "\n",
    "\n",
    "for i in range(noOfInstances):\n",
    "    for j in range(len(feature_names)):\n",
    "        X.iloc[i, j] = X.iloc[i, j] * math.sqrt(W_all[i][j])\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, shuffle = True)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X = scaler.fit_transform(X)\n",
    "#knn = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform')\n",
    "model = xgb.XGBClassifier(scale_pos_weight = scale_pos_weight)\n",
    "scores = cross_val_score(model, X, Y, cv = 10, scoring='roc_auc')\n",
    "print(np.mean(scores))\n",
    "\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "# knn.fit(X_train,Y_train) \n",
    "# Y_pred = knn.predict(X_test)\n",
    "# print(accuracy_score(Y_test,Y_pred))\n",
    "# print(f1_score(Y_test,Y_pred))\n",
    "    \n",
    "# knn = KNeighborsClassifier(n_neighbors = 5, weights = compute_weights)\n",
    "# knn.fit(X_train,Y_train) \n",
    "# Y_pred = knn.predict(X_test)\n",
    "# print(accuracy_score(Y_test,Y_pred))\n",
    "# print(f1_score(Y_test,Y_pred))\n",
    "# print(\"---------------\")\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 5, weights = compute_weights)\n",
    "# scores = cross_val_score(knn, X, Y, cv=20)\n",
    "# print(scores)\n",
    "# print(np.mean(scores))\n",
    "\n",
    "# cv = LeaveOneOut()\n",
    "# s = 0\n",
    "# for train_ix, test_ix in cv.split(X):\n",
    "#     knn = KNeighborsClassifier(n_neighbors = 5, weights = compute_weights)\n",
    "#     X_train, X_test = X.to_numpy()[train_ix, :], X.to_numpy()[test_ix, :]\n",
    "#     Y_train, Y_test = Y[train_ix], Y[test_ix]\n",
    "#     knn.fit(X_train, Y_train)  \n",
    "#     pred = knn.predict(X_test)\n",
    "#     accuracy_classifier = accuracy_score(Y_test, pred)\n",
    "#     s = s + accuracy_classifier\n",
    "# print(s / len(X))\n",
    "\n",
    "def euclidian_distance(a, b):\n",
    "    dim = len(a)\n",
    "    distance = 0\n",
    "    for i in range(dim):\n",
    "        distance += abs(a[i] - b[i])**2  \n",
    "    distance = math.sqrt(distance)    \n",
    "    return distance\n",
    "\n",
    "\n",
    "def knn_predict_non_weighted(X_train, X_test, Y_train, Y_test, k):\n",
    "    \n",
    "    from collections import Counter\n",
    "    Y_hat_test = []\n",
    "\n",
    "    for test_point in X_test:\n",
    "        distances = []\n",
    "        for train_point in X_train:\n",
    "            dim = len(train_point)\n",
    "            distance = 0\n",
    "            for i in range(dim):\n",
    "                distance = distance + math.sqrt((train_point[i] - test_point[i]) * (train_point[i] - test_point[i]))\n",
    "            distances.append(distance)\n",
    "\n",
    "        df_dists = pd.DataFrame(data = distances, columns = ['dist'], \n",
    "                                index = Y_train.index)\n",
    "\n",
    "        df_nn = df_dists.sort_values(by=['dist'], axis = 0)[:k]\n",
    "\n",
    "        counter = Counter(Y_train[df_nn.index])\n",
    "\n",
    "        prediction = counter.most_common()[0][0]\n",
    "\n",
    "        Y_hat_test.append(prediction)\n",
    "        \n",
    "    return Y_hat_test\n",
    "\n",
    "\n",
    "\n",
    "def knn_predict_weighted(X_train, X_test, Y_train, Y_test, weights, k):\n",
    "    \n",
    "    from collections import Counter\n",
    "    Y_hat_test = []\n",
    "\n",
    "    for test_point in X_test:\n",
    "        distances = []\n",
    "        for train_point in X_train:\n",
    "            dim = len(train_point)\n",
    "            distance = 0\n",
    "            for i in range(dim):\n",
    "                distance = distance + math.sqrt(weights[i] * (train_point[i] - test_point[i]) * (train_point[i] - test_point[i]))\n",
    "            distances.append(distance)\n",
    "\n",
    "        df_dists = pd.DataFrame(data = distances, columns = ['dist'], \n",
    "                                index = Y_train.index)\n",
    "\n",
    "        df_nn = df_dists.sort_values(by=['dist'], axis = 0)[:k]\n",
    "\n",
    "        counter = Counter(Y_train[df_nn.index])\n",
    "\n",
    "        prediction = counter.most_common()[0][0]\n",
    "\n",
    "        Y_hat_test.append(prediction)\n",
    "        \n",
    "    return Y_hat_test\n",
    "\n",
    "outputs = pd.DataFrame(columns = ['ACC_WS', 'F1_WS', 'ROC_WS'])\n",
    "acc_non_weighted = 0\n",
    "acc_weighted = 0\n",
    "f1_non_weighted = 0\n",
    "f1_weighted = 0\n",
    "roc_non_weighted = 0\n",
    "roc_weighted = 0\n",
    "for i in range(number_runnings):\n",
    "    output = []\n",
    "    #print('------------------')\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, shuffle = True)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    Y_hat_test = knn_predict_non_weighted(X_train, X_test, Y_train, Y_test, k=5)\n",
    "    #print(f1_score(Y_test, Y_hat_test))\n",
    "    acc_non_weighted += accuracy_score(Y_test, Y_hat_test)\n",
    "    output.append(accuracy_score(Y_test, Y_hat_test))\n",
    "    f1_non_weighted += f1_score(Y_test, Y_hat_test)\n",
    "    output.append(f1_score(Y_test, Y_hat_test))\n",
    "    roc_non_weighted += roc_auc_score(Y_test, Y_hat_test)\n",
    "    output.append(roc_auc_score(Y_test, Y_hat_test))\n",
    "    \n",
    "\n",
    "    outputs.loc[i] = output\n",
    "\n",
    "\n",
    "acc_non_weighted = acc_non_weighted / number_runnings\n",
    "acc_weighted = acc_weighted / number_runnings\n",
    "f1_non_weighted = f1_non_weighted / number_runnings\n",
    "f1_weighted = f1_weighted / number_runnings\n",
    "roc_non_weighted = roc_non_weighted / number_runnings\n",
    "roc_weighted = roc_weighted / number_runnings\n",
    "output = []\n",
    "output.append(acc_non_weighted)\n",
    "output.append(f1_non_weighted)\n",
    "output.append(roc_non_weighted)\n",
    "\n",
    "\n",
    "outputs.loc[number_runnings] = output\n",
    "\n",
    "outputs.to_excel(\"Rankings_accuraciesFetalMSPMS_NoMinMax_Shap_\" + str(number_runnings) + \".xlsx\")\n",
    "\n",
    "# cv = LeaveOneOut()\n",
    "# s_non_weighted = 0\n",
    "# s_weighted = 0\n",
    "# for train_ix, test_ix in cv.split(X):\n",
    "#     X_train, X_test = X.to_numpy()[train_ix, :], X.to_numpy()[test_ix, :]\n",
    "#     Y_train, Y_test = Y[train_ix], Y[test_ix]\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "#     Y_hat_test_non_weighted = knn_predict_non_weighted(X_train, X_test, Y_train, Y_test, k=5)\n",
    "#     s_non_weighted = s_non_weighted + accuracy_score(Y_test, Y_hat_test_non_weighted)\n",
    "#     Y_hat_test_weighted = knn_predict_weighted(X_train, X_test, Y_train, Y_test, k=5)\n",
    "#     s_weighted = s_weighted + accuracy_score(Y_test, Y_hat_test_non_weighted)\n",
    "# print(s_non_weighted / len(X))\n",
    "# print(s_weighted / len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a20711-d909-495b-83f7-1dc654e08aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b260da5-aaba-4768-85e9-f2f880cbe120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef04c3-7a75-46d0-abce-00b17b4379af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18716396-545f-4967-8f32-a1586965bd25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
